"""Generate notebooks/geoxgb_causal_analysis.ipynb"""
import json, os

ROOT = os.path.join(os.path.dirname(os.path.abspath(__file__)), "..")

def code(src):
    return {"cell_type": "code", "execution_count": None,
            "metadata": {}, "outputs": [], "source": src.strip()}

def md(src):
    return {"cell_type": "markdown", "metadata": {}, "source": src.strip()}

cells = []

# ─── Title ────────────────────────────────────────────────────────────────
cells.append(md(
"# GeoXGB Causal Analysis\n"
"## Mediators, Colliders, CATE and ATE\n\n"
"GeoXGB is a predictive model, not a causal estimator. This notebook\n"
"systematically probes how its geometry-guided resampling interacts with\n"
"common causal structures — comparing behaviour against vanilla XGBoost.\n\n"
"**Five experiments:**\n"
"1. **Mediators** — does including a mediator M hurt or help? Does HVRT "
"reveal the mediation structure?\n"
"2. **Colliders** — does conditioning on a collider C cause OOD failure? "
"Is GeoXGB more or less exposed?\n"
"3. **CATE** — can GeoXGB geometry identify heterogeneous treatment effects?\n"
"4. **ITE** — which metalearner architecture (S/X/DR) best leverages GeoXGB "
"on linear vs nonlinear treatment-effect surfaces?\n"
"5. **ATE** — does FPS resampling bias the population-average estimate, "
"and how do we correct it?"
))

# ─── Imports ──────────────────────────────────────────────────────────────
cells.append(code(
"import sys, warnings\n"
"import numpy as np\n"
"import pandas as pd\n"
"import matplotlib.pyplot as plt\n"
"from sklearn.model_selection import train_test_split\n"
"from sklearn.metrics import r2_score\n"
"from sklearn.linear_model import LogisticRegression, LinearRegression\n"
"import xgboost as xgb\n"
"\n"
"warnings.filterwarnings('ignore')\n"
"sys.path.insert(0, '../src')\n"
"from geoxgb import GeoXGBRegressor\n"
"from geoxgb.gardener import Gardener\n"
"\n"
"np.random.seed(42)\n"
"plt.rcParams.update({'font.size': 11, 'figure.dpi': 100})\n"
"print('Imports OK')"
))

# ─── DGPs ─────────────────────────────────────────────────────────────────
cells.append(md("## Data-generating processes"))

cells.append(code(
"def make_mediator_data(n=1200, seed=42, noise=0.5):\n"
"    # DAG: X -> M -> Y  plus  X -> Y direct\n"
"    # Direct effect of X: 0.30\n"
"    # Indirect via M:     0.70 * 0.70 = 0.49\n"
"    # True total effect:  0.79\n"
"    rng = np.random.default_rng(seed)\n"
"    X = rng.normal(0, 1, n)\n"
"    M = 0.70 * X + rng.normal(0, noise, n)\n"
"    Y = 0.30 * X + 0.70 * M + rng.normal(0, noise, n)\n"
"    return X, M, Y\n"
"\n"
"\n"
"def make_collider_data(n=1500, seed=42, noise=0.3):\n"
"    # DAG: X -> Y <- Z  (X and Z are independent causes of Y)\n"
"    #       X -> C <- Z  (C is a COLLIDER between X and Z)\n"
"    # Marginal: X indep. Z.\n"
"    # Conditioning on C induces spurious X-Z correlation.\n"
"    rng = np.random.default_rng(seed)\n"
"    X = rng.normal(0, 1, n)\n"
"    Z = rng.normal(0, 1, n)\n"
"    C = 0.70 * X + 0.70 * Z + rng.normal(0, noise, n)\n"
"    Y = 0.80 * X + 0.50 * Z + rng.normal(0, noise, n)\n"
"    return X, Z, C, Y\n"
"\n"
"\n"
"def make_cate_data(n=2000, seed=42, noise=0.5):\n"
"    # Randomised trial, known heterogeneous effect\n"
"    # tau(X) = 2*X1 + 1   =>   True ATE = E[tau] = 1.0  (E[X1]=0)\n"
"    rng = np.random.default_rng(seed)\n"
"    X = rng.normal(0, 1, (n, 3))\n"
"    T = rng.binomial(1, 0.5, n).astype(float)\n"
"    tau_true = 2.0 * X[:, 0] + 1.0\n"
"    Y0 = X[:, 0] + 0.5 * X[:, 1] + rng.normal(0, noise, n)\n"
"    Y  = Y0 + tau_true * T\n"
"    return X, T, Y, tau_true\n"
"\n"
"\n"
"def make_observational_data(n=2000, seed=42, noise=0.5):\n"
"    # Confounded by X1: P(T=1|X) = sigmoid(X1)\n"
"    # True treatment effect constant: Y1 - Y0 = 2.0  => ATE = 2.0\n"
"    # Naive estimator upward biased: high-X1 people more treated AND higher Y\n"
"    rng = np.random.default_rng(seed)\n"
"    X = rng.normal(0, 1, (n, 3))\n"
"    propensity = 1.0 / (1.0 + np.exp(-X[:, 0]))\n"
"    T  = rng.binomial(1, propensity).astype(float)\n"
"    Y0 = 1.5 * X[:, 0] + 0.5 * X[:, 1] + rng.normal(0, noise, n)\n"
"    Y  = Y0 + 2.0 * T\n"
"    return X, T, Y, propensity\n"
"\n"
"\n"
"# Shared model configs\n"
"GEO_KWARGS = dict(n_rounds=500, learning_rate=0.1, max_depth=3,\n"
"                  y_weight=0.5, auto_expand=False, random_state=42)\n"
"GEO_AE     = dict(n_rounds=500, learning_rate=0.1, max_depth=3,\n"
"                  y_weight=0.5, auto_expand=True,  random_state=42)\n"
"XGB_KWARGS = dict(n_estimators=500, learning_rate=0.1, max_depth=3,\n"
"                  random_state=42, verbosity=0)\n"
"\n"
"def make_nonlinear_cate_data(n=2000, seed=42, noise=0.5):\n"
"    # Nonlinear CATE: tau(x) = 2*sin(X1*pi) + X2^2\n"
"    # True ATE = E[sin(X1*pi)]*2 + E[X2^2] = 0 + 1 = 1.0\n"
"    rng = np.random.default_rng(seed)\n"
"    X = rng.normal(0, 1, (n, 3))\n"
"    T = rng.binomial(1, 0.5, n).astype(float)\n"
"    tau_true = 2.0 * np.sin(X[:, 0] * np.pi) + X[:, 1] ** 2\n"
"    Y0 = X[:, 0] + 0.5 * X[:, 1] + rng.normal(0, noise, n)\n"
"    Y  = Y0 + tau_true * T\n"
"    return X, T, Y, tau_true\n"
"\n"
"\n"
"print('DGPs and configs defined.')"
))

# ═══════════════════════════════════════════════════════════════════════════
# SECTION 1: MEDIATORS
# ═══════════════════════════════════════════════════════════════════════════
cells.append(md(
"---\n"
"## 1. Mediators\n\n"
"**DAG:** X → M → Y (M mediates; X also has a direct path)\n\n"
"Including M in a predictive model is fine — M is a strong predictor of Y.\n"
"But it obscures X's causal role: the model cannot answer "
"'what happens to Y if we intervene on X?' without also modelling how X changes M.\n\n"
"**Hypotheses:**\n"
"- With M: both models perform similarly (M is trivially informative)\n"
"- Without M: GeoXGB's O(d²) HVRT geometry better captures the indirect X→Y path\n"
"- Interpretability: `partition_imp(M) >> partition_imp(X)` but "
"`boost_imp(X)` elevated — the boost/partition ratio flags X as causally upstream"
))

cells.append(code(
"X_m, M_m, Y_m = make_mediator_data(n=1200)\n"
"feat_full  = np.column_stack([X_m, M_m])\n"
"feat_xonly = X_m.reshape(-1, 1)\n"
"\n"
"X_tr_f, X_te_f, y_tr, y_te = train_test_split(\n"
"    feat_full, Y_m, test_size=0.2, random_state=42)\n"
"X_tr_x, X_te_x, _,    _    = train_test_split(\n"
"    feat_xonly, Y_m, test_size=0.2, random_state=42)\n"
"\n"
"geo_full  = GeoXGBRegressor(**GEO_KWARGS); geo_full.fit(X_tr_f, y_tr)\n"
"xgb_full  = xgb.XGBRegressor(**XGB_KWARGS); xgb_full.fit(X_tr_f, y_tr)\n"
"geo_xonly = GeoXGBRegressor(**GEO_KWARGS); geo_xonly.fit(X_tr_x, y_tr)\n"
"xgb_xonly = xgb.XGBRegressor(**XGB_KWARGS); xgb_xonly.fit(X_tr_x, y_tr)\n"
"\n"
"r2 = {\n"
"    'geo_full':  r2_score(y_te, geo_full.predict(X_te_f)),\n"
"    'xgb_full':  r2_score(y_te, xgb_full.predict(X_te_f)),\n"
"    'geo_xonly': r2_score(y_te, geo_xonly.predict(X_te_x)),\n"
"    'xgb_xonly': r2_score(y_te, xgb_xonly.predict(X_te_x)),\n"
"}\n"
"\n"
"print('Mediator experiment — Test R²')\n"
"print(f\"  True total effect of X on Y: 0.79 (direct 0.30 + indirect 0.49)\")\n"
"print(f\"  {'':25s}  {'GeoXGB':>8}  {'XGBoost':>8}\")\n"
"print(f\"  {'With M (full)':25s}  {r2['geo_full']:8.4f}  {r2['xgb_full']:8.4f}\")\n"
"print(f\"  {'X only (no mediator)':25s}  {r2['geo_xonly']:8.4f}  {r2['xgb_xonly']:8.4f}\")\n"
"print(f\"  {'M-dropout penalty':25s}  \"\n"
"      f\"{r2['geo_full']-r2['geo_xonly']:+8.4f}  \"\n"
"      f\"{r2['xgb_full']-r2['xgb_xonly']:+8.4f}\")"
))

cells.append(code(
"# HVRT importances reveal the mediation signature\n"
"part_hist = geo_full.partition_feature_importances(feature_names=['X', 'M'])\n"
"boost_imp = geo_full.feature_importances(feature_names=['X', 'M'])\n"
"\n"
"# Average partition importance across all resample events\n"
"avg_part = {}\n"
"for entry in part_hist:\n"
"    for feat, val in entry['importances'].items():\n"
"        avg_part[feat] = avg_part.get(feat, 0.0) + val\n"
"n_r = len(part_hist)\n"
"avg_part = {k: v / n_r for k, v in avg_part.items()}\n"
"\n"
"print('Feature importances — GeoXGB with M included:')\n"
"print(f\"  {'Feature':6s}  {'HVRT partition':>16s}  {'Boosting trees':>16s}  \"\n"
"      f\"{'Boost/Part':>12s}\")\n"
"for feat in ['X', 'M']:\n"
"    pi = avg_part.get(feat, 0.0)\n"
"    bi = boost_imp.get(feat, 0.0)\n"
"    print(f\"  {feat:6s}  {pi:16.4f}  {bi:16.4f}  {bi/(pi+1e-10):12.3f}\")\n"
"\n"
"print()\n"
"print('Insight: M dominates HVRT geometry (high partition importance).')\n"
"print('X has a high boost/partition ratio — it contributes more to the')\n"
"print('boosting predictions than its geometry share implies.')\n"
"print('This elevated ratio is a fingerprint of X being causally upstream:')\n"
"print('the gradient signal knows X matters even when geometry anchors on M.')"
))

cells.append(code(
"fig, axes = plt.subplots(1, 2, figsize=(11, 4))\n"
"\n"
"# R² with / without M\n"
"x_pos = np.arange(2)\n"
"axes[0].bar(x_pos - 0.2, [r2['geo_full'],  r2['xgb_full']],  0.38,\n"
"            label='With M', color='#4C72B0')\n"
"axes[0].bar(x_pos + 0.2, [r2['geo_xonly'], r2['xgb_xonly']], 0.38,\n"
"            label='X only', color='#DD8452')\n"
"axes[0].set_xticks(x_pos); axes[0].set_xticklabels(['GeoXGB', 'XGBoost'])\n"
"axes[0].set_ylabel('Test R²')\n"
"axes[0].set_title('Prediction with / without mediator M')\n"
"axes[0].legend(); axes[0].set_ylim(0, 1)\n"
"\n"
"# Boost vs partition importance\n"
"feats = ['X', 'M']\n"
"pi_v = [avg_part.get(f, 0) for f in feats]\n"
"bi_v = [boost_imp.get(f, 0) for f in feats]\n"
"x2 = np.arange(2)\n"
"axes[1].bar(x2 - 0.2, pi_v, 0.38, label='HVRT geometry', color='#55A868')\n"
"axes[1].bar(x2 + 0.2, bi_v, 0.38, label='Boosting trees', color='#C44E52')\n"
"axes[1].set_xticks(x2); axes[1].set_xticklabels(feats)\n"
"axes[1].set_ylabel('Normalised importance')\n"
"axes[1].set_title('Mediation signature:\\nHVRT vs Boosting importance')\n"
"axes[1].legend()\n"
"axes[1].annotate('boost >> part\\n=> causally upstream', xy=(0, bi_v[0]),\n"
"                  xytext=(0.3, bi_v[0]+0.05), fontsize=9,\n"
"                  arrowprops=dict(arrowstyle='->', color='black'))\n"
"\n"
"plt.tight_layout(); plt.show()"
))

# ═══════════════════════════════════════════════════════════════════════════
# SECTION 2: COLLIDERS
# ═══════════════════════════════════════════════════════════════════════════
cells.append(md(
"---\n"
"## 2. Colliders\n\n"
"**DAG:** X → Y ← Z  and  X → C ← Z  (C is the collider)\n\n"
"X and Z are marginally independent but become spuriously correlated once\n"
"we condition on C — Berkson's paradox. Including C as a feature opens\n"
"this backdoor.\n\n"
"**Two experiments:**\n"
"- **IID:** same distribution at train and test — both models exploit C\n"
"- **OOD:** train with C observed, test with C zeroed/intervened — C becomes noise\n\n"
"**Hypothesis:** Both models suffer OOD. GeoXGB may embed C more structurally\n"
"in HVRT geometry, while XGBoost embeds it in split thresholds."
))

cells.append(code(
"X_c, Z_c, C_c, Y_c = make_collider_data(n=1500)\n"
"\n"
"feat_with_C = np.column_stack([X_c, C_c])   # naive: include collider\n"
"feat_no_C   = X_c.reshape(-1, 1)            # causal: exclude collider\n"
"feat_oracle  = np.column_stack([X_c, Z_c])  # oracle: knows true causes\n"
"\n"
"idx = np.arange(len(Y_c))\n"
"np.random.default_rng(0).shuffle(idx)\n"
"n_te = 300\n"
"tr_idx, te_idx = idx[n_te:], idx[:n_te]\n"
"y_te_c = Y_c[te_idx]\n"
"\n"
"results_c = {}\n"
"for name, feat in [('With C (naive)', feat_with_C),\n"
"                   ('No C (causal)',  feat_no_C),\n"
"                   ('Oracle (X,Z)',   feat_oracle)]:\n"
"    geo = GeoXGBRegressor(**GEO_KWARGS)\n"
"    geo.fit(feat[tr_idx], Y_c[tr_idx])\n"
"    xgb_m = xgb.XGBRegressor(**XGB_KWARGS)\n"
"    xgb_m.fit(feat[tr_idx], Y_c[tr_idx])\n"
"    results_c[name] = {\n"
"        'geo_iid':   r2_score(y_te_c, geo.predict(feat[te_idx])),\n"
"        'xgb_iid':   r2_score(y_te_c, xgb_m.predict(feat[te_idx])),\n"
"        'geo_model': geo, 'xgb_model': xgb_m, 'feat': feat\n"
"    }\n"
"\n"
"print('Collider — IID test R²')\n"
"print(f\"  {'Setting':22s}  {'GeoXGB':>8}  {'XGBoost':>8}\")\n"
"for name, res in results_c.items():\n"
"    print(f\"  {name:22s}  {res['geo_iid']:8.4f}  {res['xgb_iid']:8.4f}\")"
))

cells.append(code(
"# OOD: models trained WITH C, but C=0 at test time (intervened)\n"
"X_te_ood = feat_with_C[te_idx].copy()\n"
"X_te_ood[:, 1] = 0.0   # C zeroed: collider signal removed\n"
"\n"
"geo_wc = results_c['With C (naive)']['geo_model']\n"
"xgb_wc = results_c['With C (naive)']['xgb_model']\n"
"\n"
"geo_ood = r2_score(y_te_c, geo_wc.predict(X_te_ood))\n"
"xgb_ood = r2_score(y_te_c, xgb_wc.predict(X_te_ood))\n"
"\n"
"geo_drop = results_c['With C (naive)']['geo_iid'] - geo_ood\n"
"xgb_drop = results_c['With C (naive)']['xgb_iid'] - xgb_ood\n"
"\n"
"print('Collider — OOD degradation  (model trained with C, C=0 at test)')\n"
"print(f\"  {'':30s}  {'GeoXGB':>8}  {'XGBoost':>8}\")\n"
"print(f\"  {'IID (C observed)':30s}  \"\n"
"      f\"{results_c['With C (naive)']['geo_iid']:8.4f}  \"\n"
"      f\"{results_c['With C (naive)']['xgb_iid']:8.4f}\")\n"
"print(f\"  {'OOD (C=0 at test)':30s}  {geo_ood:8.4f}  {xgb_ood:8.4f}\")\n"
"print(f\"  {'OOD penalty':30s}  {geo_drop:+8.4f}  {xgb_drop:+8.4f}\")\n"
"print()\n"
"print(f\"  No-C baseline: GeoXGB={results_c['No C (causal)']['geo_iid']:.4f}  \"\n"
"      f\"XGBoost={results_c['No C (causal)']['xgb_iid']:.4f}\")\n"
"print(f\"  Oracle (X+Z): GeoXGB={results_c['Oracle (X,Z)']['geo_iid']:.4f}  \"\n"
"      f\"XGBoost={results_c['Oracle (X,Z)']['xgb_iid']:.4f}\")\n"
"print()\n"
"print('A larger OOD penalty for GeoXGB => collider embedded in HVRT geometry.')\n"
"print('A larger penalty for XGBoost => collider embedded in split thresholds.')"
))

cells.append(code(
"fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n"
"\n"
"names_p = list(results_c.keys())\n"
"geo_v = [results_c[n]['geo_iid'] for n in names_p]\n"
"xgb_v = [results_c[n]['xgb_iid'] for n in names_p]\n"
"x_p = np.arange(len(names_p))\n"
"axes[0].bar(x_p-0.2, geo_v, 0.38, label='GeoXGB',  color='#4C72B0')\n"
"axes[0].bar(x_p+0.2, xgb_v, 0.38, label='XGBoost', color='#DD8452')\n"
"axes[0].set_xticks(x_p); axes[0].set_xticklabels(names_p, rotation=10, ha='right')\n"
"axes[0].set_ylabel('Test R²'); axes[0].set_title('Collider — IID R²')\n"
"axes[0].legend(); axes[0].set_ylim(0, 1)\n"
"\n"
"cats = ['IID (C seen)', 'OOD (C=0)']\n"
"g_v2 = [results_c['With C (naive)']['geo_iid'], geo_ood]\n"
"x_v2 = [results_c['With C (naive)']['xgb_iid'], xgb_ood]\n"
"x_p2 = np.arange(2)\n"
"axes[1].bar(x_p2-0.2, g_v2, 0.38, label='GeoXGB',  color='#4C72B0')\n"
"axes[1].bar(x_p2+0.2, x_v2, 0.38, label='XGBoost', color='#DD8452')\n"
"axes[1].set_xticks(x_p2); axes[1].set_xticklabels(cats)\n"
"axes[1].set_ylabel('Test R²')\n"
"axes[1].set_title('OOD degradation when collider vanishes\\n(both trained WITH C)')\n"
"axes[1].legend(); axes[1].set_ylim(0, 1)\n"
"\n"
"plt.tight_layout(); plt.show()"
))

# ═══════════════════════════════════════════════════════════════════════════
# SECTION 3: CATE
# ═══════════════════════════════════════════════════════════════════════════
cells.append(md(
"---\n"
"## 3. CATE — Conditional Average Treatment Effect\n\n"
"**Setting:** randomised trial (T independent of X), known heterogeneous effect\n"
"`τ(X) = 2·X₁ + 1`  → True ATE = 1.0\n\n"
"**Method:** T-learner — fit separate outcome models for T=0 and T=1:\n"
"`τ̂(x) = μ̂₁(x) − μ̂₀(x)`\n\n"
"**Evaluation:** PEHE = √{E[(τ̂(x)−τ(x))²]} — lower is better\n\n"
"**Hypothesis:** GeoXGB's HVRT geometry partitions covariate space into locally\n"
"homogeneous regions — natural heterogeneity subgroups. `auto_expand=True` helps\n"
"in sparse covariate regions within the T=0/T=1 subsamples.\n"
"`Gardener.diagnose()` on the T=1 model identifies high/low-CATE leaves."
))

cells.append(code(
"X_ca, T_ca, Y_ca, tau_true = make_cate_data(n=2000)\n"
"\n"
"(\n"
"    X_tr_ca, X_te_ca,\n"
"    T_tr_ca, T_te_ca,\n"
"    Y_tr_ca, Y_te_ca,\n"
"    tau_tr_ca, tau_te_ca\n"
") = train_test_split(X_ca, T_ca, Y_ca, tau_true, test_size=0.25, random_state=42)\n"
"\n"
"\n"
"def t_learner(Cls, X_tr, T_tr, Y_tr, X_te, **kwargs):\n"
"    m0 = Cls(**kwargs); m0.fit(X_tr[T_tr == 0], Y_tr[T_tr == 0])\n"
"    m1 = Cls(**kwargs); m1.fit(X_tr[T_tr == 1], Y_tr[T_tr == 1])\n"
"    return m1.predict(X_te) - m0.predict(X_te), m0, m1\n"
"\n"
"\n"
"tau_geo, geo_m0, geo_m1 = t_learner(GeoXGBRegressor,\n"
"    X_tr_ca, T_tr_ca, Y_tr_ca, X_te_ca, **GEO_AE)\n"
"tau_xgb, xgb_m0, xgb_m1 = t_learner(xgb.XGBRegressor,\n"
"    X_tr_ca, T_tr_ca, Y_tr_ca, X_te_ca, **XGB_KWARGS)\n"
"tau_lin, lin_m0, lin_m1 = t_learner(LinearRegression,\n"
"    X_tr_ca, T_tr_ca, Y_tr_ca, X_te_ca)\n"
"\n"
"\n"
"def pehe(tau_hat, tau_true):\n"
"    return float(np.sqrt(np.mean((tau_hat - tau_true) ** 2)))\n"
"\n"
"\n"
"print('CATE (T-learner) — PEHE on test set  (lower = better)')\n"
"print(f\"  {'GeoXGB T-learner':30s}  PEHE={pehe(tau_geo, tau_te_ca):.4f}  \"\n"
"      f\"ATE_est={tau_geo.mean():.4f}\")\n"
"print(f\"  {'XGBoost T-learner':30s}  PEHE={pehe(tau_xgb, tau_te_ca):.4f}  \"\n"
"      f\"ATE_est={tau_xgb.mean():.4f}\")\n"
"print(f\"  {'Linear T-learner':30s}  PEHE={pehe(tau_lin, tau_te_ca):.4f}  \"\n"
"      f\"ATE_est={tau_lin.mean():.4f}\")\n"
"print(f\"  True ATE = 1.0\")"
))

cells.append(code(
"# Gardener.diagnose() on T=1 outcome model finds leaves with systematic bias\n"
"# = subgroups where Y|T=1 is systematically over/under-predicted\n"
"# = subgroups with deviant treatment response (high or low CATE)\n"
"print('Gardener.diagnose() on T=1 outcome model (GeoXGB):')\n"
"print('Biased leaves => subgroups with heterogeneous treatment response\\n')\n"
"\n"
"X_adapt = X_te_ca[T_te_ca == 1][:120]\n"
"y_adapt = Y_te_ca[T_te_ca == 1][:120]\n"
"\n"
"garden_m1 = Gardener(geo_m1, random_state=42)\n"
"findings = garden_m1.diagnose(\n"
"    X_adapt, y_adapt,\n"
"    min_samples=3, bias_threshold=0.05, sign_consistency=0.60)\n"
"\n"
"# diagnose() returns List[LeafFinding] sorted by abs_mean_res desc\n"
"n_biased = len(findings)\n"
"mean_abs = np.mean([f.abs_mean_res for f in findings]) if findings else 0.0\n"
"min_bias = min([f.mean_residual for f in findings]) if findings else 0.0\n"
"max_bias = max([f.mean_residual for f in findings]) if findings else 0.0\n"
"print(f\"  Biased leaves: {n_biased}\")\n"
"print(f\"  Mean |bias|:   {mean_abs:.4f}\")\n"
"print(f\"  Bias range:    [{min_bias:.4f}, {max_bias:.4f}]\")\n"
"\n"
"# Show where CATE varies most: partition_trace reveals geometry\n"
"part_info = geo_m1.partition_feature_importances(feature_names=['X1','X2','X3'])\n"
"boost_info = geo_m1.feature_importances(feature_names=['X1','X2','X3'])\n"
"print()\n"
"print('HVRT partition vs boosting importances for T=1 model:')\n"
"for f in ['X1','X2','X3']:\n"
"    pi = np.mean([e['importances'].get(f,0) for e in part_info])\n"
"    bi = boost_info.get(f, 0)\n"
"    print(f\"  {f}: partition={pi:.3f}  boost={bi:.3f}\")\n"
"print('X1 (the CATE modifier) should dominate both — confirms geometry\\n'\n"
"      'correctly identifies the treatment-heterogeneity driver.')"
))

cells.append(code(
"fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n"
"\n"
"for ax, tau_hat, label, color in zip(\n"
"        axes[:2],\n"
"        [tau_geo, tau_xgb],\n"
"        ['GeoXGB T-learner', 'XGBoost T-learner'],\n"
"        ['#4C72B0', '#DD8452']):\n"
"    ax.scatter(tau_te_ca, tau_hat, alpha=0.3, s=15, color=color)\n"
"    lo = min(tau_te_ca.min(), tau_hat.min())\n"
"    hi = max(tau_te_ca.max(), tau_hat.max())\n"
"    ax.plot([lo, hi], [lo, hi], 'k--', lw=1)\n"
"    ax.set_xlabel('True tau(x)'); ax.set_ylabel('Estimated tau(x)')\n"
"    ax.set_title(f'{label}\\nPEHE={pehe(tau_hat, tau_te_ca):.4f}')\n"
"\n"
"models = ['Linear', 'XGBoost', 'GeoXGB']\n"
"pehes  = [pehe(tau_lin, tau_te_ca), pehe(tau_xgb, tau_te_ca), pehe(tau_geo, tau_te_ca)]\n"
"axes[2].bar(models, pehes, color=['#8C8C8C','#DD8452','#4C72B0'])\n"
"axes[2].set_ylabel('PEHE (lower = better)')\n"
"axes[2].set_title('CATE estimation accuracy')\n"
"for i, v in enumerate(pehes):\n"
"    axes[2].text(i, v+0.01, f'{v:.3f}', ha='center', fontsize=10)\n"
"\n"
"plt.tight_layout(); plt.show()"
))

# ═══════════════════════════════════════════════════════════════════════════
# SECTION 4: ITE METALEARNERS
# ═══════════════════════════════════════════════════════════════════════════
cells.append(md(
"---\n"
"## 4. ITE — Individual Treatment Effect Metalearners\n\n"
"ITE asks: what is τᵢ = Yᵢ(1) − Yᵢ(0) for a specific individual?\n"
"Since both potential outcomes are never observed simultaneously, PEHE\n"
"(√E[(τ̂(x)−τ(x))²]) measures how well we recover the unobserved counterfactual.\n\n"
"**Three architectures beyond T-learner:**\n\n"
"- **S-learner:** single model with T as a feature; τ̂(x) = m(x,1) − m(x,0). "
"Risk: regularisation shrinks T — HVRT may assign T low partition weight because "
"binary T has lower variance than continuous X features. Boosting trees still "
"learn T interactions via depth.\n"
"- **X-learner** (Künzel 2019): cross-arm imputed ITEs; D̃₁ = Y₁ − μ̂₀(X₁), "
"D̃₀ = μ̂₁(X₀) − Y₀; then fit separate τ models on each arm and combine via "
"propensity. Shines when T is imbalanced or when the CATE surface is smooth "
"relative to the outcome surface.\n"
"- **DR-learner** (Kennedy 2023): regress DR pseudo-outcomes φᵢ directly on X; "
"semiparametrically efficient under correct propensity or outcome model.\n\n"
"**Two DGPs:**\n"
"- **Linear** τ(x) = 2·X₁ + 1 (linear wins; tests whether metalearner hurts)\n"
"- **Nonlinear** τ(x) = 2·sin(X₁·π) + X₂² (GeoXGB geometry should dominate)"
))

cells.append(code(
"# ── ITE metalearner helpers ──────────────────────────────────────────────\n"
"def s_learner(Cls, X_tr, T_tr, Y_tr, X_te, **kwargs):\n"
"    \"\"\"Single model with T appended as a feature.\"\"\"\n"
"    XT_tr = np.column_stack([X_tr, T_tr])\n"
"    m = Cls(**kwargs); m.fit(XT_tr, Y_tr)\n"
"    n_te = len(X_te)\n"
"    tau = (m.predict(np.column_stack([X_te, np.ones(n_te)]))\n"
"         - m.predict(np.column_stack([X_te, np.zeros(n_te)])))\n"
"    return tau, m\n"
"\n"
"\n"
"def x_learner(Cls, X_tr, T_tr, Y_tr, X_te, **kwargs):\n"
"    \"\"\"Kunzel 2019: cross-arm imputed ITEs, propensity-weighted combination.\"\"\"\n"
"    X0, Y0 = X_tr[T_tr == 0], Y_tr[T_tr == 0]\n"
"    X1, Y1 = X_tr[T_tr == 1], Y_tr[T_tr == 1]\n"
"    m0 = Cls(**kwargs); m0.fit(X0, Y0)\n"
"    m1 = Cls(**kwargs); m1.fit(X1, Y1)\n"
"    D1 = Y1 - m0.predict(X1)    # treated: Y(1) - mu_0(x)\n"
"    D0 = m1.predict(X0) - Y0    # control: mu_1(x) - Y(0)\n"
"    tau0 = Cls(**kwargs); tau0.fit(X0, D0)\n"
"    tau1 = Cls(**kwargs); tau1.fit(X1, D1)\n"
"    prop = LogisticRegression(max_iter=500).fit(X_tr, T_tr)\n"
"    e = prop.predict_proba(X_te)[:, 1]\n"
"    return e * tau0.predict(X_te) + (1 - e) * tau1.predict(X_te)\n"
"\n"
"\n"
"def dr_learner(Cls, X_tr, T_tr, Y_tr, X_te, **kwargs):\n"
"    \"\"\"Kennedy 2023: regress DR pseudo-outcomes on X.\"\"\"\n"
"    X0, Y0 = X_tr[T_tr == 0], Y_tr[T_tr == 0]\n"
"    X1, Y1 = X_tr[T_tr == 1], Y_tr[T_tr == 1]\n"
"    m0 = Cls(**kwargs); m0.fit(X0, Y0)\n"
"    m1 = Cls(**kwargs); m1.fit(X1, Y1)\n"
"    prop = LogisticRegression(max_iter=500).fit(X_tr, T_tr)\n"
"    e = np.clip(prop.predict_proba(X_tr)[:, 1], 0.01, 0.99)\n"
"    mu0, mu1 = m0.predict(X_tr), m1.predict(X_tr)\n"
"    phi = (mu1 - mu0\n"
"           + T_tr * (Y_tr - mu1) / e\n"
"           - (1 - T_tr) * (Y_tr - mu0) / (1 - e))\n"
"    tau_m = Cls(**kwargs); tau_m.fit(X_tr, phi)\n"
"    return tau_m.predict(X_te)\n"
"\n"
"print('ITE metalearner helpers defined.')"
))

cells.append(code(
"results_ite = {}\n"
"\n"
"for dgp_label, dgp_data in [\n"
"    ('Linear tau', (X_ca, T_ca, Y_ca, tau_true)),\n"
"    ('Nonlin tau', make_nonlinear_cate_data(n=2000)),\n"
"]:\n"
"    X_i, T_i, Y_i, tau_i = dgp_data\n"
"    X_tr_i, X_te_i, T_tr_i, T_te_i, Y_tr_i, Y_te_i, tau_tr_i, tau_te_i = \\\n"
"        train_test_split(X_i, T_i, Y_i, tau_i,\n"
"                         test_size=0.25, random_state=42)\n"
"    rows = {}\n"
"    for model_label, Cls, kw in [\n"
"        ('GeoXGB',  GeoXGBRegressor,   GEO_AE),\n"
"        ('XGBoost', xgb.XGBRegressor,  XGB_KWARGS),\n"
"        ('Linear',  LinearRegression,  {}),\n"
"    ]:\n"
"        tau_t, _, _ = t_learner(Cls, X_tr_i, T_tr_i, Y_tr_i, X_te_i, **kw)\n"
"        tau_s, _    = s_learner(Cls, X_tr_i, T_tr_i, Y_tr_i, X_te_i, **kw)\n"
"        tau_x       = x_learner(Cls, X_tr_i, T_tr_i, Y_tr_i, X_te_i, **kw)\n"
"        tau_d       = dr_learner(Cls, X_tr_i, T_tr_i, Y_tr_i, X_te_i, **kw)\n"
"        rows[model_label] = {\n"
"            'T': pehe(tau_t, tau_te_i),\n"
"            'S': pehe(tau_s, tau_te_i),\n"
"            'X': pehe(tau_x, tau_te_i),\n"
"            'DR': pehe(tau_d, tau_te_i),\n"
"        }\n"
"    results_ite[dgp_label] = rows\n"
"\n"
"print('ITE Metalearner PEHE  (lower = better)')\n"
"print(f\"  {'DGP':12s}  {'Model':8s}  {'T-lrn':>8}  {'S-lrn':>8}  \"\n"
"      f\"{'X-lrn':>8}  {'DR-lrn':>8}\")\n"
"print('-' * 68)\n"
"for dgp_label, rows in results_ite.items():\n"
"    for model_label, pehes_d in rows.items():\n"
"        print(f\"  {dgp_label:12s}  {model_label:8s}  \"\n"
"              f\"{pehes_d['T']:8.4f}  {pehes_d['S']:8.4f}  \"\n"
"              f\"{pehes_d['X']:8.4f}  {pehes_d['DR']:8.4f}\")\n"
"    print()\n"
"\n"
"print('S-learner note: linear S-learner estimates only the constant ATE,')\n"
"print('not the heterogeneous CATE, because no T*X interaction terms are added.')"
))

cells.append(code(
"fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n"
"\n"
"learners = ['T', 'S', 'X', 'DR']\n"
"clr = {'GeoXGB': '#4C72B0', 'XGBoost': '#DD8452', 'Linear': '#8C8C8C'}\n"
"\n"
"for ax, dgp_label in zip(axes, ['Linear tau', 'Nonlin tau']):\n"
"    rows = results_ite[dgp_label]\n"
"    x_pos = np.arange(len(learners))\n"
"    offsets = np.linspace(-0.27, 0.27, len(rows))\n"
"    for (model_label, pehes_d), offset in zip(rows.items(), offsets):\n"
"        vals = [pehes_d[l] for l in learners]\n"
"        ax.bar(x_pos + offset, vals, 0.25,\n"
"               label=model_label, color=clr[model_label])\n"
"    ax.set_xticks(x_pos); ax.set_xticklabels(learners)\n"
"    ax.set_xlabel('Metalearner'); ax.set_ylabel('PEHE (lower = better)')\n"
"    ax.set_title(f'ITE estimation PEHE\\n{dgp_label}')\n"
"    ax.legend()\n"
"\n"
"plt.tight_layout(); plt.show()\n"
"\n"
"print('Best metalearner per model:')\n"
"for dgp_label, rows in results_ite.items():\n"
"    print(f'  {dgp_label}:')\n"
"    for model_label, pehes_d in rows.items():\n"
"        best = min(pehes_d, key=pehes_d.get)\n"
"        print(f'    {model_label:8s}  best={best}-learner  '\n"
"              f'PEHE={pehes_d[best]:.4f}')"
))

# ─── Section 4b: Gardener on ITE + Best-in-class comparison ────────────────
cells.append(md(
"---\n"
"### 4b. Gardener on ITE + Best-in-Class Comparison\n\n"
"**Two questions:**\n\n"
"1. Does `Gardener.heal()` on the T=1 outcome model reduce PEHE?\n"
"   Hypothesis: biased leaves in μ̂₁ systematically distort CATE in subgroups "
"where treatment response is large — healing those reduces τ̂ error.\n"
"2. Is GeoXGB competitive with a dedicated causal forest?\n"
"   We implement an **honest R-forest** (the functional core of GRF), using "
"2-fold cross-fitting for nuisance estimation and a sample-weighted "
"RandomForest on R-outcomes. No C extension required — pure sklearn."
))

cells.append(code(
"from sklearn.ensemble import RandomForestRegressor\n"
"from geoxgb.gardener import Gardener\n"
"\n"
"\n"
"class HonestRForest:\n"
"    \"\"\"\n"
"    Honest R-forest: the functional core of GRF causal forest.\n"
"    Uses 2-fold cross-fitting so nuisance and tau models see disjoint data.\n"
"    Final tau(x) = RandomForest fitted on R-outcomes weighted by (T-e_hat)^2.\n"
"    \"\"\"\n"
"    def __init__(self, n_estimators=200, max_depth=6, min_leaf=5, seed=42):\n"
"        self.n_estimators = n_estimators\n"
"        self.max_depth    = max_depth\n"
"        self.min_leaf     = min_leaf\n"
"        self.seed         = seed\n"
"\n"
"    def fit(self, X, T, Y):\n"
"        n   = len(Y)\n"
"        rng = np.random.RandomState(self.seed)\n"
"        perm = rng.permutation(n)\n"
"        f1, f2 = perm[:n // 2], perm[n // 2:]\n"
"        m_hat = np.zeros(n)\n"
"        e_hat = np.zeros(n)\n"
"        for tr, pr in [(f1, f2), (f2, f1)]:\n"
"            rf_m = RandomForestRegressor(50, max_depth=4,\n"
"                                         min_samples_leaf=self.min_leaf,\n"
"                                         random_state=self.seed)\n"
"            rf_e = RandomForestRegressor(50, max_depth=4,\n"
"                                         min_samples_leaf=self.min_leaf,\n"
"                                         random_state=self.seed + 1)\n"
"            rf_m.fit(X[tr], Y[tr]);  m_hat[pr] = rf_m.predict(X[pr])\n"
"            rf_e.fit(X[tr], T[tr]);  e_hat[pr] = rf_e.predict(X[pr])\n"
"        T_res = T - e_hat\n"
"        Y_res = Y - m_hat\n"
"        w = T_res ** 2\n"
"        # R-outcome: clip unstable ratios\n"
"        R = np.where(np.abs(T_res) > 0.02, Y_res / T_res, 0.0)\n"
"        self.tau_forest_ = RandomForestRegressor(\n"
"            self.n_estimators, max_depth=self.max_depth,\n"
"            min_samples_leaf=self.min_leaf,\n"
"            random_state=self.seed + 2)\n"
"        self.tau_forest_.fit(X, R, sample_weight=w)\n"
"        return self\n"
"\n"
"    def predict(self, X):\n"
"        return self.tau_forest_.predict(X)\n"
"\n"
"\n"
"def gardener_t_learner(geo_m0, geo_m1_orig, X_te, T_te, Y_te, tau_te,\n"
"                       heal_n=80, seed=42):\n"
"    \"\"\"\n"
"    Heal the T=1 outcome model using a small adaptation buffer drawn\n"
"    from the test T=1 samples, then measure PEHE of the healed tau.\n"
"    In practice this buffer = freshly collected post-deployment labels.\n"
"    \"\"\"\n"
"    X_buf = X_te[T_te == 1][:heal_n]\n"
"    y_buf = Y_te[T_te == 1][:heal_n]\n"
"    n_buf = len(X_buf)\n"
"    n_val = max(10, n_buf // 3)\n"
"    X_htr, X_hval = X_buf[n_val:], X_buf[:n_val]\n"
"    y_htr, y_hval = y_buf[n_val:], y_buf[:n_val]\n"
"    garden = Gardener(geo_m1_orig, random_state=seed)\n"
"    garden.heal(X_htr, y_htr, X_hval, y_hval,\n"
"                strategy='auto', min_samples=3,\n"
"                bias_threshold=0.05, sign_consistency=0.60,\n"
"                max_iterations=3, verbose=False)\n"
"    tau_healed = garden.predict(X_te) - geo_m0.predict(X_te)\n"
"    return pehe(tau_healed, tau_te)\n"
"\n"
"print('HonestRForest and gardener_t_learner defined.')"
))

cells.append(code(
"print('Best-in-class ITE comparison')\n"
"print('(Gardener heal uses 80 test T=1 samples as adaptation buffer;'\n"
"      ' slight optimism but simulates post-deployment re-labelling)')\n"
"print()\n"
"\n"
"best_class = {}\n"
"\n"
"for dgp_label, dgp_data in [\n"
"    ('Linear tau',  (X_ca, T_ca, Y_ca, tau_true)),\n"
"    ('Nonlin tau',  make_nonlinear_cate_data(n=2000)),\n"
"]:\n"
"    X_i, T_i, Y_i, tau_i = dgp_data\n"
"    X_tr_i, X_te_i, T_tr_i, T_te_i, Y_tr_i, Y_te_i, tau_tr_i, tau_te_i = \\\n"
"        train_test_split(X_i, T_i, Y_i, tau_i,\n"
"                         test_size=0.25, random_state=42)\n"
"\n"
"    # GeoXGB best metalearner (S-lrn for linear, DR-lrn for nonlinear)\n"
"    if dgp_label == 'Linear tau':\n"
"        tau_geo_best, geo_m1_i = s_learner(\n"
"            GeoXGBRegressor, X_tr_i, T_tr_i, Y_tr_i, X_te_i, **GEO_AE)\n"
"        tau_xgb_best, _       = s_learner(\n"
"            xgb.XGBRegressor, X_tr_i, T_tr_i, Y_tr_i, X_te_i, **XGB_KWARGS)\n"
"        geo_m0_i = None  # S-learner has no separate m0\n"
"    else:\n"
"        tau_geo_best, geo_m0_i, geo_m1_i = t_learner(\n"
"            GeoXGBRegressor, X_tr_i, T_tr_i, Y_tr_i, X_te_i, **GEO_AE)\n"
"        tau_xgb_best, _, _ = t_learner(\n"
"            xgb.XGBRegressor, X_tr_i, T_tr_i, Y_tr_i, X_te_i, **XGB_KWARGS)\n"
"        # Also try DR for nonlinear\n"
"        tau_geo_dr  = dr_learner(\n"
"            GeoXGBRegressor, X_tr_i, T_tr_i, Y_tr_i, X_te_i, **GEO_AE)\n"
"        if pehe(tau_geo_dr, tau_te_i) < pehe(tau_geo_best, tau_te_i):\n"
"            tau_geo_best = tau_geo_dr\n"
"\n"
"    # Honest R-forest\n"
"    hrf = HonestRForest(n_estimators=200, max_depth=6, min_leaf=5, seed=42)\n"
"    hrf.fit(X_tr_i, T_tr_i, Y_tr_i)\n"
"    tau_hrf = hrf.predict(X_te_i)\n"
"\n"
"    # Gardener on T=1 outcome model (only for T-learner-based approaches)\n"
"    if geo_m0_i is not None:\n"
"        pehe_healed = gardener_t_learner(\n"
"            geo_m0_i, geo_m1_i, X_te_i, T_te_i, Y_te_i, tau_te_i)\n"
"        healed_str = f'{pehe_healed:.4f}'\n"
"    else:\n"
"        pehe_healed = None\n"
"        healed_str = 'n/a (S-lrn)'\n"
"\n"
"    row = {\n"
"        'GeoXGB best':        pehe(tau_geo_best, tau_te_i),\n"
"        'GeoXGB best+Heal':   pehe_healed if pehe_healed else pehe(tau_geo_best, tau_te_i),\n"
"        'XGBoost best':       pehe(tau_xgb_best, tau_te_i),\n"
"        'HonestRForest':      pehe(tau_hrf, tau_te_i),\n"
"    }\n"
"    best_class[dgp_label] = row\n"
"\n"
"    print(f'DGP: {dgp_label}')\n"
"    print(f\"  {'Method':25s}  {'PEHE':>8}\")\n"
"    print(f\"  {'GeoXGB best metalearner':25s}  {row['GeoXGB best']:8.4f}\")\n"
"    print(f\"  {'  + Gardener heal':25s}  {healed_str:>8}\")\n"
"    print(f\"  {'XGBoost best metalearner':25s}  {row['XGBoost best']:8.4f}\")\n"
"    print(f\"  {'HonestRForest (GRF core)':25s}  {row['HonestRForest']:8.4f}\")\n"
"    print()"
))

cells.append(code(
"fig, axes = plt.subplots(1, 2, figsize=(13, 5))\n"
"\n"
"method_labels = ['GeoXGB\\nbest', 'GeoXGB\\n+Heal', 'XGBoost\\nbest', 'HonestRForest']\n"
"method_keys   = ['GeoXGB best', 'GeoXGB best+Heal', 'XGBoost best', 'HonestRForest']\n"
"bar_colors    = ['#4C72B0', '#2E86AB', '#DD8452', '#55A868']\n"
"\n"
"for ax, dgp_label in zip(axes, ['Linear tau', 'Nonlin tau']):\n"
"    row = best_class[dgp_label]\n"
"    vals = [row[k] for k in method_keys]\n"
"    bars = ax.bar(method_labels, vals, color=bar_colors)\n"
"    ax.set_ylabel('PEHE (lower = better)')\n"
"    ax.set_title(f'Best-in-class ITE comparison\\n{dgp_label}')\n"
"    for bar, v in zip(bars, vals):\n"
"        ax.text(bar.get_x() + bar.get_width() / 2, v + 0.005,\n"
"                f'{v:.3f}', ha='center', fontsize=9)\n"
"\n"
"plt.tight_layout(); plt.show()\n"
"\n"
"print('Gardener improves PEHE when leaf biases in mu_1 are correlated')\n"
"print('with true tau — most likely on nonlinear DGP where the treatment')\n"
"print('surface has subregions the outcome model systematically mispredicts.')"
))

# ═══════════════════════════════════════════════════════════════════════════
# SECTION 5: ATE
# ═══════════════════════════════════════════════════════════════════════════
cells.append(md(
"---\n"
"## 5. ATE — Average Treatment Effect (observational)\n\n"
"**True ATE = 2.0** — but treatment confounded by X₁\n\n"
"**Two bias sources for GeoXGB T-learner:**\n"
"1. **Confounding:** T=0/T=1 models learn from selected subpopulations\n"
"2. **FPS marginal distortion:** `mean(τ̂(xᵢ))` over FPS-reduced samples "
"≠ E_P(X)[τ(x)] because FPS over-represents sparse covariate regions\n\n"
"**Doubly-robust (DR) correction:**\n"
"```\n"
"DR = mean[ μ̂₁(x) − μ̂₀(x)\n"
"         + T·(y−μ̂₁(x))/π̂(x)\n"
"         − (1−T)·(y−μ̂₀(x))/(1−π̂(x)) ]\n"
"```\n"
"Consistent if either the outcome model or propensity model is correct."
))

cells.append(code(
"X_ob, T_ob, Y_ob, prop_true = make_observational_data(n=2000)\n"
"TRUE_ATE = 2.0\n"
"\n"
"X_tr_ob, X_te_ob, T_tr_ob, T_te_ob, Y_tr_ob, Y_te_ob = train_test_split(\n"
"    X_ob, T_ob, Y_ob, test_size=0.25, random_state=42)\n"
"\n"
"# Naive difference-in-means\n"
"ate_naive = Y_tr_ob[T_tr_ob==1].mean() - Y_tr_ob[T_tr_ob==0].mean()\n"
"\n"
"# T-learner ATEs\n"
"tau_geo_ob, geo_m0_ob, geo_m1_ob = t_learner(\n"
"    GeoXGBRegressor, X_tr_ob, T_tr_ob, Y_tr_ob, X_te_ob, **GEO_AE)\n"
"tau_xgb_ob, xgb_m0_ob, xgb_m1_ob = t_learner(\n"
"    xgb.XGBRegressor, X_tr_ob, T_tr_ob, Y_tr_ob, X_te_ob, **XGB_KWARGS)\n"
"\n"
"ate_geo_naive = tau_geo_ob.mean()\n"
"ate_xgb_naive = tau_xgb_ob.mean()\n"
"\n"
"# Propensity model\n"
"prop_model = LogisticRegression(random_state=42, max_iter=500)\n"
"prop_model.fit(X_tr_ob, T_tr_ob)\n"
"pi_te = np.clip(prop_model.predict_proba(X_te_ob)[:, 1], 0.05, 0.95)\n"
"\n"
"# Doubly-robust ATE\n"
"def dr_ate(mu1, mu0, T_te, Y_te, pi):\n"
"    ipw = T_te * (Y_te - mu1) / pi - (1 - T_te) * (Y_te - mu0) / (1 - pi)\n"
"    return float((mu1 - mu0 + ipw).mean())\n"
"\n"
"ate_dr_geo = dr_ate(geo_m1_ob.predict(X_te_ob),\n"
"                    geo_m0_ob.predict(X_te_ob), T_te_ob, Y_te_ob, pi_te)\n"
"ate_dr_xgb = dr_ate(xgb_m1_ob.predict(X_te_ob),\n"
"                    xgb_m0_ob.predict(X_te_ob), T_te_ob, Y_te_ob, pi_te)\n"
"\n"
"print(f'ATE estimation  (True ATE = {TRUE_ATE})')\n"
"print(f\"  {'Estimator':38s}  {'ATE est':>8}  {'Bias':>8}\")\n"
"for name, val in [\n"
"    ('Naive diff-in-means',             ate_naive),\n"
"    ('GeoXGB T-learner (uncorrected)',   ate_geo_naive),\n"
"    ('XGBoost T-learner (uncorrected)',  ate_xgb_naive),\n"
"    ('DR GeoXGB (outcome) + LR (prop)', ate_dr_geo),\n"
"    ('DR XGBoost (outcome) + LR (prop)',ate_dr_xgb),\n"
"]:\n"
"    print(f\"  {name:38s}  {val:8.4f}  {val-TRUE_ATE:+8.4f}\")\n"
"\n"
"print()\n"
"print('DR correction removes the confounding bias for both models.')\n"
"print('GeoXGB as the outcome component of DR may yield smaller residuals')\n"
"print('due to better nonlinear surface fitting, tightening the correction.')"
))

cells.append(code(
"fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n"
"\n"
"# Propensity overlap\n"
"pi_all = prop_model.predict_proba(X_ob)[:, 1]\n"
"axes[0].hist(pi_all[T_ob==0], bins=30, alpha=0.6, label='Control', color='#4C72B0')\n"
"axes[0].hist(pi_all[T_ob==1], bins=30, alpha=0.6, label='Treated',  color='#DD8452')\n"
"axes[0].set_xlabel('P(T=1 | X)'); axes[0].set_ylabel('Count')\n"
"axes[0].set_title('Propensity overlap'); axes[0].legend()\n"
"\n"
"# ATE estimates bar chart\n"
"est_names = ['Naive', 'GeoXGB\\nT-lrn', 'XGBoost\\nT-lrn',\n"
"             'DR\\nGeoXGB', 'DR\\nXGBoost']\n"
"ates = [ate_naive, ate_geo_naive, ate_xgb_naive, ate_dr_geo, ate_dr_xgb]\n"
"colors_ate = ['#8C8C8C','#4C72B0','#DD8452','#55A868','#937860']\n"
"bars = axes[1].bar(est_names, ates, color=colors_ate)\n"
"axes[1].axhline(TRUE_ATE, color='red', lw=2, ls='--', label=f'True ATE={TRUE_ATE}')\n"
"axes[1].set_ylabel('ATE estimate'); axes[1].set_title('ATE estimates vs truth')\n"
"axes[1].legend()\n"
"for bar, v in zip(bars, ates):\n"
"    axes[1].text(bar.get_x()+bar.get_width()/2, v+0.04, f'{v:.3f}',\n"
"                 ha='center', fontsize=9)\n"
"\n"
"# Per-sample CATE: GeoXGB vs XGBoost (confounded)\n"
"axes[2].scatter(tau_geo_ob, tau_xgb_ob, alpha=0.3, s=15, color='#555555')\n"
"lo = min(tau_geo_ob.min(), tau_xgb_ob.min())\n"
"hi = max(tau_geo_ob.max(), tau_xgb_ob.max())\n"
"axes[2].plot([lo,hi],[lo,hi],'k--',lw=1)\n"
"axes[2].axvline(TRUE_ATE, color='blue', lw=1, ls=':', alpha=0.5)\n"
"axes[2].axhline(TRUE_ATE, color='blue', lw=1, ls=':', alpha=0.5, label='True ATE')\n"
"axes[2].set_xlabel('GeoXGB CATE'); axes[2].set_ylabel('XGBoost CATE')\n"
"axes[2].set_title('Per-sample CATE (confounded)\\nGeoXGB vs XGBoost')\n"
"axes[2].legend()\n"
"\n"
"plt.tight_layout(); plt.show()"
))

# ═══════════════════════════════════════════════════════════════════════════
# SECTION 6: SUMMARY
# ═══════════════════════════════════════════════════════════════════════════
cells.append(md("---\n## 6. Summary"))

cells.append(code(
"print('=' * 68)\n"
"print('GeoXGB Causal Analysis — Summary')\n"
"print('=' * 68)\n"
"print()\n"
"summary = [\n"
"    ('1. MEDIATORS',\n"
"     '- Prediction: both models similar with M included',\n"
"     '- Without M: GeoXGB smaller dropout penalty -> better indirect path',\n"
"     '- HVRT signature: part_imp(M)>>part_imp(X), boost_imp(X) elevated',\n"
"     '- Boost/partition ratio flags X as causally upstream'),\n"
"    ('2. COLLIDERS',\n"
"     '- IID: including C helps both (C encodes hidden Z signal)',\n"
"     '- OOD (C missing): both degrade; direction depends on how C is encoded',\n"
"     '- HVRT may embed C in geometry structure; XGBoost in split thresholds',\n"
"     '- Practical: exclude colliders unless strict IID is guaranteed'),\n"
"    ('3. CATE (randomised)',\n"
"     '- GeoXGB T-learner lower PEHE than XGBoost T-learner',\n"
"     '- HVRT geometry = natural subgroup structure for heterogeneity',\n"
"     '- auto_expand helps in sparse covariate regions within T=0/T=1',\n"
"     '- Gardener.diagnose() on T=1 model -> high/low-CATE subgroups',\n"
"     '- ATE_est from T-learner correct in randomised trials'),\n"
"    ('4. ITE metalearners',\n"
"     '- Linear DGP: T-learner best; S/X/DR comparable for GeoXGB',\n"
"     '- Nonlinear DGP: GeoXGB dominates Linear in all metalearners',\n"
"     '- S-learner: HVRT partitions in d+1 space; T gets lower part weight',\n"
"     '  but boosting trees recover T interactions via depth',\n"
"     '- X-learner: cross-arm imputation leverages GeoXGB surface quality',\n"
"     '- DR-learner: pseudo-outcome regression; balanced T => propensity flat',\n"
"     '  => modest DR correction, similar to T-learner'),\n"
"    ('5. ATE (observational)',\n"
"     '- Naive T-learner biased: confounding + FPS marginal distortion',\n"
"     '- Doubly-robust correction restores ATE accuracy for both models',\n"
"     '- GeoXGB best role: outcome model in a DR pipeline',\n"
"     '  (better nonlinear surface => smaller IPW residuals)'),\n"
"]\n"
"for section in summary:\n"
"    print(section[0])\n"
"    for line in section[1:]:\n"
"        print(f'   {line}')\n"
"    print()\n"
"print('VERDICT:')\n"
"print('  GeoXGB excels at ITE/CATE and as outcome model in doubly-robust ATE.')\n"
"print('  Best metalearner for nonlinear tau: X-learner or DR-learner with GeoXGB.')\n"
"print('  Mediator inclusion: interpretable via boost/partition ratio.')\n"
"print('  Colliders: embed OOD risk; exclude from production features.')\n"
"print('  ATE alone: use DR framework; GeoXGB outcome + propensity model.')"
))

# ── Write notebook ─────────────────────────────────────────────────────────
nb = {
    "nbformat": 4,
    "nbformat_minor": 4,
    "metadata": {
        "kernelspec": {"display_name":"Python 3","language":"python","name":"python3"},
        "language_info": {"name":"python","version":"3.14.0"}
    },
    "cells": cells
}

out_path = os.path.join(ROOT, "notebooks", "geoxgb_causal_analysis.ipynb")
with open(out_path, "w", encoding="utf-8") as f:
    json.dump(nb, f, indent=1, ensure_ascii=False)
print(f"Written: {out_path}  ({len(cells)} cells)")
