"""
Y-Assignment Alternatives Benchmark
=====================================

In hvrt_resample(), synthetic samples generated by KDE expansion are assigned
y-values (gradient signals) via _knn_assign_y: k=3 inverse-distance weighted
k-NN in HVRT z-score space.

HVRT partitions are *hyperplane-homogeneous* — each partition is a decision-tree
leaf in z-score space, and all samples within it are geometrically coherent.
Synthetic samples are generated from WITHIN a partition's KDE, so their natural
y-reference is the real samples in that SAME partition, not k global nearest
neighbours which may cross partition boundaries.

Three alternatives tested
--------------------------
  knn (current)   — k=3 global inverse-distance weighted k-NN in z-space
  part-mean       — mean y of all real reduced samples in the same partition
  part-idw        — inverse-distance weighted average within partition only;
                    falls back to partition mean if only 1 member
  nearest-part    — for each synthetic sample, find the nearest partition
                    centroid (in z-space); use that partition's mean y
                    (handles edge case where synthetic sample's own partition
                    has no reduced samples after FPS)

Datasets: make_classification (n=1000) and Friedman #1 (n=1000)
Metric: AUC / R2 on 20% holdout, mean of 5 seeds.
n_rounds=1000, expand_ratio=0.3 (forces KDE expansion so assignment matters).
"""
import warnings
import numpy as np
from sklearn.datasets import make_classification, make_friedman1
from sklearn.metrics import roc_auc_score, r2_score
from sklearn.model_selection import train_test_split
from sklearn.neighbors import NearestNeighbors

import geoxgb._resampling as _res_mod
from geoxgb import GeoXGBClassifier, GeoXGBRegressor

warnings.filterwarnings("ignore")

RANDOM_SEEDS = [0, 1, 2, 3, 4]
N_SAMPLES    = 1000
N_ROUNDS     = 1000
EXPAND_RATIO = 0.30   # force expansion so y-assignment has real impact
COL_W        = 18


# ---------------------------------------------------------------------------
# Alternative y-assignment functions (same signature as _knn_assign_y)
# ---------------------------------------------------------------------------

def _part_mean_assign_y(X_syn, X_red, y_red, hvrt_model):
    """Partition-mean: each synthetic sample gets the mean y of its partition."""
    X_syn_z = hvrt_model._to_z(X_syn).astype(np.float32)
    X_red_z = hvrt_model._to_z(X_red).astype(np.float32)

    syn_leaves = hvrt_model.tree_.apply(X_syn_z)
    red_leaves = hvrt_model.tree_.apply(X_red_z)

    leaf_y = {}
    for leaf, y in zip(red_leaves, y_red):
        leaf_y.setdefault(leaf, []).append(y)

    global_mean = y_red.mean()
    return np.array([
        np.mean(leaf_y[leaf]) if leaf in leaf_y else global_mean
        for leaf in syn_leaves
    ])


def _part_idw_assign_y(X_syn, X_red, y_red, hvrt_model):
    """Intra-partition inverse-distance weighted; falls back to partition mean."""
    X_syn_z = hvrt_model._to_z(X_syn).astype(np.float32)
    X_red_z = hvrt_model._to_z(X_red).astype(np.float32)

    syn_leaves = hvrt_model.tree_.apply(X_syn_z)
    red_leaves = hvrt_model.tree_.apply(X_red_z)

    leaf_idx = {}
    for i, leaf in enumerate(red_leaves):
        leaf_idx.setdefault(leaf, []).append(i)

    global_mean = y_red.mean()
    y_syn = np.empty(len(X_syn))

    for i, (leaf, x_z) in enumerate(zip(syn_leaves, X_syn_z)):
        idxs = leaf_idx.get(leaf)
        if not idxs:
            y_syn[i] = global_mean
            continue
        if len(idxs) == 1:
            y_syn[i] = y_red[idxs[0]]
            continue
        dists = np.linalg.norm(X_red_z[idxs] - x_z, axis=1)
        w = 1.0 / (dists + 1e-10)
        w /= w.sum()
        y_syn[i] = np.dot(w, y_red[idxs])

    return y_syn


def _nearest_part_assign_y(X_syn, X_red, y_red, hvrt_model):
    """
    Nearest-partition centroid: find the closest partition centroid in z-space
    and use that partition's mean y.  Handles the edge case where a synthetic
    sample's own partition has no reduced members after FPS.
    """
    X_syn_z = hvrt_model._to_z(X_syn).astype(np.float32)
    X_red_z = hvrt_model._to_z(X_red).astype(np.float32)

    syn_leaves = hvrt_model.tree_.apply(X_syn_z)
    red_leaves = hvrt_model.tree_.apply(X_red_z)

    # Build per-leaf centroids and mean y from reduced real samples
    leaf_centroid = {}
    leaf_y_list   = {}
    for i, leaf in enumerate(red_leaves):
        leaf_centroid.setdefault(leaf, []).append(X_red_z[i])
        leaf_y_list.setdefault(leaf, []).append(y_red[i])

    centroids = {leaf: np.mean(vs, axis=0) for leaf, vs in leaf_centroid.items()}
    leaf_ymean = {leaf: np.mean(ys)         for leaf, ys  in leaf_y_list.items()}

    centroid_arr  = np.array(list(centroids.values()), dtype=np.float32)
    centroid_keys = list(centroids.keys())
    nn = NearestNeighbors(n_neighbors=1, algorithm="auto").fit(centroid_arr)

    global_mean = y_red.mean()
    y_syn = np.empty(len(X_syn))

    for i, (leaf, x_z) in enumerate(zip(syn_leaves, X_syn_z)):
        if leaf in leaf_ymean:
            y_syn[i] = leaf_ymean[leaf]
        else:
            # Synthetic sample's partition has no reduced members: nearest centroid
            _, nn_idx = nn.kneighbors(x_z.reshape(1, -1))
            nearest_leaf = centroid_keys[nn_idx[0, 0]]
            y_syn[i] = leaf_ymean.get(nearest_leaf, global_mean)

    return y_syn


# ---------------------------------------------------------------------------
# Benchmark harness: monkeypatch _knn_assign_y and evaluate
# ---------------------------------------------------------------------------

ASSIGN_FNS = {
    "knn (current)":  _res_mod._knn_assign_y,   # original — stored before any patch
    "part-mean":      _part_mean_assign_y,
    "part-idw":       _part_idw_assign_y,
    "nearest-part":   _nearest_part_assign_y,
}

# Store the original so we can restore it
_ORIGINAL_KNN = _res_mod._knn_assign_y


def _evaluate_seed(seed, task, assign_fn):
    """Fit a GeoXGB model with a patched y-assignment function and return score."""
    _res_mod._knn_assign_y = assign_fn

    rng = np.random.default_rng(seed)
    if task == "clf":
        X, y = make_classification(
            n_samples=N_SAMPLES, n_features=10, n_informative=6,
            n_redundant=2, random_state=seed,
        )
        X = X.astype(np.float64)
        X_tr, X_te, y_tr, y_te = train_test_split(
            X, y, test_size=0.20, stratify=y, random_state=seed,
        )
        m = GeoXGBClassifier(
            n_rounds=N_ROUNDS, expand_ratio=EXPAND_RATIO,
            auto_expand=False, random_state=seed,
        )
        m.fit(X_tr, y_tr)
        score = roc_auc_score(y_te, m.predict_proba(X_te)[:, 1])
    else:
        X, y = make_friedman1(n_samples=N_SAMPLES, n_features=10,
                               noise=1.0, random_state=seed)
        X = X.astype(np.float64)
        X_tr, X_te, y_tr, y_te = train_test_split(
            X, y, test_size=0.20, random_state=seed,
        )
        m = GeoXGBRegressor(
            n_rounds=N_ROUNDS, expand_ratio=EXPAND_RATIO,
            auto_expand=False, random_state=seed,
        )
        m.fit(X_tr, y_tr)
        score = r2_score(y_te, m.predict(X_te))

    _res_mod._knn_assign_y = _ORIGINAL_KNN
    return score


# ---------------------------------------------------------------------------
# Run
# ---------------------------------------------------------------------------

print(f"Y-Assignment Alternatives  |  n={N_SAMPLES}, expand_ratio={EXPAND_RATIO},"
      f" {len(RANDOM_SEEDS)} seeds, n_rounds={N_ROUNDS}")
print()

for task, label, metric in [
    ("clf", "Classification", "AUC"),
    ("reg", "Regression / Friedman #1", "R2 "),
]:
    print(f"{'='*60}")
    print(f"  {label}")
    print(f"{'='*60}")

    results = {name: [] for name in ASSIGN_FNS}

    for name, fn in ASSIGN_FNS.items():
        for seed in RANDOM_SEEDS:
            results[name].append(_evaluate_seed(seed, task, fn))
        print(f"  {name:<18} done")

    print()
    means = {n: np.mean(v) for n, v in results.items()}
    stds  = {n: np.std(v)  for n, v in results.items()}
    best  = max(means.values())
    base  = means["knn (current)"]

    print(f"  {'Method':<{COL_W}}  {f'Mean {metric}':>10}  {'Std':>6}  {'vs knn':>10}")
    print("  " + "-" * (COL_W + 34))
    for name in ASSIGN_FNS:
        marker  = " *" if means[name] == best else "  "
        delta   = means[name] - base if name != "knn (current)" else 0.0
        delta_s = f"{delta:+.4f}" if name != "knn (current)" else "baseline"
        print(f"  {name:<{COL_W}}  {means[name]:>10.4f}  {stds[name]:>6.4f}  "
              f"{delta_s:>10}{marker}")
    print()

# Restore just in case
_res_mod._knn_assign_y = _ORIGINAL_KNN
print("=== DONE ===")
