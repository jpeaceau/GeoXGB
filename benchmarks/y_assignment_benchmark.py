"""
Y-Assignment Strategy Benchmark
================================

In hvrt_resample(), synthetic samples generated by KDE expansion are assigned
y-values (gradient signals) via _knn_assign_y.  GeoXGB v0.1.1 supports three
strategies via the ``assignment_strategy`` parameter:

  'knn'      — global inverse-distance weighted k-NN (k=3) in HVRT z-space.
               Safe in all regimes; used as baseline.

  'part-idw' — intra-partition IDW using the main HVRT tree.  Each synthetic
               sample is assigned the IDW-mean of reduced real samples in the
               same HVRT partition leaf.  Falls back to global k-NN when a
               partition has no reduced representatives.  Best when HVRT has
               many well-populated partitions.

  'auto'     — selects 'part-idw' when X_red spans >= 50 unique HVRT
               partitions (enough members per partition for reliable IDW),
               falls back to 'knn' otherwise.  Conservative default that
               matches HVRT's own bandwidth='auto' philosophy.

Background
----------
HVRT partitions are hyperplane-homogeneous — each partition is a decision-tree
leaf in z-score space, so samples within it are geometrically coherent.
Synthetic samples are generated from within-partition KDE, making their natural
y-reference the real samples in that SAME partition.  'part-idw' exploits this
property when partitions are well-populated (>= 50 unique partitions in X_red).

Earlier tests (part-idw at expand_ratio=0.3, n=1000) showed:
  Classification: +0.0009 AUC vs k-NN
  Regression:     +0.0050 R2  vs k-NN

Datasets: make_classification (n=2000) and Friedman #1 (n=2000)
Metric: AUC / R2 on 20% holdout, mean of 5 seeds.
n_rounds=1000, expand_ratio=0.3 (forces KDE expansion so assignment matters).
n=2000 is used so the HVRT tree has enough samples to create >= 50 partitions,
triggering the 'auto' -> 'part-idw' path.
"""
import warnings
import numpy as np
from sklearn.datasets import make_classification, make_friedman1
from sklearn.metrics import roc_auc_score, r2_score
from sklearn.model_selection import train_test_split

from geoxgb import GeoXGBClassifier, GeoXGBRegressor

warnings.filterwarnings("ignore")

RANDOM_SEEDS  = [0, 1, 2, 3, 4]
N_SAMPLES     = 2000
N_ROUNDS      = 1000
EXPAND_RATIO  = 0.30   # force expansion so y-assignment has real impact
COL_W         = 12

STRATEGIES = ["knn", "part-idw", "auto"]


def _evaluate_seed(seed, task, strategy):
    """Fit a GeoXGB model with a given assignment_strategy and return score."""
    if task == "clf":
        X, y = make_classification(
            n_samples=N_SAMPLES, n_features=10, n_informative=6,
            n_redundant=2, random_state=seed,
        )
        X = X.astype(np.float64)
        X_tr, X_te, y_tr, y_te = train_test_split(
            X, y, test_size=0.20, stratify=y, random_state=seed,
        )
        m = GeoXGBClassifier(
            n_rounds=N_ROUNDS, expand_ratio=EXPAND_RATIO,
            auto_expand=False, random_state=seed,
            assignment_strategy=strategy,
        )
        m.fit(X_tr, y_tr)
        return roc_auc_score(y_te, m.predict_proba(X_te)[:, 1])
    else:
        X, y = make_friedman1(n_samples=N_SAMPLES, n_features=10,
                              noise=1.0, random_state=seed)
        X = X.astype(np.float64)
        X_tr, X_te, y_tr, y_te = train_test_split(
            X, y, test_size=0.20, random_state=seed,
        )
        m = GeoXGBRegressor(
            n_rounds=N_ROUNDS, expand_ratio=EXPAND_RATIO,
            auto_expand=False, random_state=seed,
            assignment_strategy=strategy,
        )
        m.fit(X_tr, y_tr)
        return r2_score(y_te, m.predict(X_te))


print(f"Y-Assignment Strategy Benchmark  |  n={N_SAMPLES}, expand_ratio={EXPAND_RATIO},"
      f" {len(RANDOM_SEEDS)} seeds, n_rounds={N_ROUNDS}")
print()

for task, label, metric in [
    ("clf", "Classification", "AUC"),
    ("reg", "Regression / Friedman #1", "R2 "),
]:
    print(f"{'='*60}")
    print(f"  {label}")
    print(f"{'='*60}")

    results = {s: [] for s in STRATEGIES}

    for strategy in STRATEGIES:
        for seed in RANDOM_SEEDS:
            results[strategy].append(_evaluate_seed(seed, task, strategy))
        print(f"  {strategy:<12} done")

    print()
    means = {s: np.mean(v) for s, v in results.items()}
    stds  = {s: np.std(v)  for s, v in results.items()}
    best  = max(means.values())
    base  = means["knn"]

    print(f"  {'Strategy':<{COL_W}}  {f'Mean {metric}':>10}  {'Std':>6}  {'vs knn':>10}")
    print("  " + "-" * (COL_W + 34))
    for s in STRATEGIES:
        marker  = " *" if means[s] == best else "  "
        delta   = means[s] - base if s != "knn" else 0.0
        delta_s = f"{delta:+.4f}" if s != "knn" else "baseline"
        print(f"  {s:<{COL_W}}  {means[s]:>10.4f}  {stds[s]:>6.4f}  "
              f"{delta_s:>10}{marker}")
    print()

print("=== DONE ===")
